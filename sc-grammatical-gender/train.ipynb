{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Grammatical Gender From Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"it\" # \"de\" or \"it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path=\"./.env\")\n",
    "\n",
    "BASE_DIR = os.getenv(\"BASE_DIR\")\n",
    "NON_DEBIASED_DIR = os.getenv(\"NON_DEBIASED_DIR\")\n",
    "DEBIASED_DIR = os.getenv(\"DEBIASED_DIR\")\n",
    "\n",
    "NON_DEBIASED_PATH = f\"{NON_DEBIASED_DIR}/{LANGUAGE}\"\n",
    "DEBIASED_PATH = f\"{DEBIASED_DIR}/sc/{LANGUAGE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import io\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def load_embeddings_and_vocab(filename):\n",
    "    embeddings = np.load(filename + '.npy')\n",
    "    with codecs.open(filename + '.vocab', 'r', 'utf-8') as f_embed:\n",
    "        vocab = f_embed.read().split()\n",
    "\n",
    "    word2id = {w: i for i, w in enumerate(vocab)}\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "    return vocab, embeddings, word2id, id2word\n",
    "\n",
    "def normalize(embeddings):\n",
    "    # normalize vectors\n",
    "    norms = np.apply_along_axis(LA.norm, 1, embeddings)\n",
    "    embeddings = embeddings / norms[:, np.newaxis]\n",
    "    return embeddings\n",
    "\n",
    "def load_and_normalize(filename):\n",
    "    vocab, embeddings, word2id, id2word = load_embeddings_and_vocab(filename)\n",
    "    embeddings = normalize(embeddings)\n",
    "    return vocab, embeddings, word2id, id2word\n",
    "\n",
    "def load_embeddings_and_normalize(filename):\n",
    "    embeddings = np.load(filename + '.npy')\n",
    "    embeddings = normalize(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def save_embeddings(filename, vocab, embeddings):\n",
    "    np.save(filename + '.npy', embeddings)\n",
    "    with codecs.open(filename + '.vocab', 'w', 'utf-8') as f_embed:\n",
    "        for w in vocab:\n",
    "            f_embed.write(w + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, my_embeddings, my_word2id, my_id2word = load_and_normalize(NON_DEBIASED_PATH)\n",
    "\n",
    "assert len(vocab) == len(my_embeddings) == len(my_word2id) == len(my_id2word)\n",
    "assert len(vocab) == len(set(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stimuli List for experiments along with the number of iterations required for removing grammatical gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if LANGUAGE == \"de\":\n",
    "\n",
    "# German\n",
    "    num_iter = 20\n",
    "    \n",
    "    masc = ['zorn', 'streit', 'rand', 'vertrag', 'tod', 'globus', 'auftrag', 'winter'] # 'widersacher' because the same meaning as 'gegener'\n",
    "    fem =  ['wut', 'auseinandersetzung', 'grenze', 'vereinbarung', 'tragödie', 'welt', 'aufgabe', 'jahreszeit'] # 'gegener' is taken out\n",
    "    man = ['mann', 'junge', 'vater', 'männlich', 'großvater', 'ehemann', 'sohn', 'onkel']\n",
    "    wom = ['mädchen', 'weiblich', 'tante', 'tochter', 'ehefrau', 'frau', 'mutter', 'großmutter']\n",
    "\n",
    "    sci = ['astronomie', 'mathematik', 'chemie', 'physik', 'biologie', 'geologie', 'ingenieurswissenschaften', 'statistik', 'biophysik', 'biochemie', 'ökologie', 'mikrobiologie', 'algebra', 'geometrie', 'telekommunikation', 'computer', 'astrophysik', 'informatik'] # 'informatik' instead of 'bioingenieurwesen'\n",
    "    hum = ['philosophie', 'kunst', 'geschichte', 'musik', 'geisteswissenschaften', 'psychologie', 'soziologie', 'geographie', 'anthropologie', 'theologie', 'linguistik', 'journalismus', 'archäologie', 'tanz', 'zeichnung', 'malerei', 'sprachwissenschaften', 'literaturwissenschaften']\n",
    "    \n",
    "    car =  ['verwaltung', 'berufstätigkeit', 'unternehmen', 'gehalt', 'büro', 'karriere', 'geschäft', 'management']\n",
    "    fam = ['haus', 'eltern', 'kinder', 'familie', 'hochzeit', 'ehe', 'verwandte', 'cousins']\n",
    "\n",
    "    boy = ['johannes', 'lukas', 'daniel', 'paul', 'thomas', 'benjamin', 'felix', 'christopher', 'maximilian']\n",
    "    girl = ['julia', 'michaela', 'anna', 'laura', 'sofie', 'sarah', 'lisa', 'jessica', 'sabrina']\n",
    "\n",
    "    flo = ['orchidee', 'rose', 'narzisse', 'flieder', 'tulpe', 'gänseblümchen', 'lilie', 'veilchen', 'magnolie']\n",
    "    ins = ['ameise', 'floh', 'spinne', 'wanze', 'fliege', 'tarantel', 'biene', 'kakerlake', 'mücke']\n",
    "\n",
    "    instr = ['cello', 'gitarre', 'laute', 'posaune', 'banjo', 'klarinette', 'mundharmonika', 'mandoline', 'trompete', 'fagott', 'trommel', 'harfe', 'glocke', 'geige', 'cembalo', 'klavier', 'bratsche', 'flöte', 'horn', 'saxophon', 'violine']\n",
    "    wep =  ['keule', 'waffe', 'rakete', 'speer', 'axt', 'dolch', 'harpune', 'pistole', 'dynamit', 'beil', 'gewehr', 'panzer', 'bombe', 'schusswaffe', 'messer', 'schrotflinte', 'tränengas', 'kanone', 'granate', 'schleuder', 'peitsche']\n",
    "\n",
    "    plez = ['freiheit', 'gesundheit', 'liebe', 'frieden', 'jubel', 'freund', 'himmel', 'treue', 'vergnügen', 'diamant', 'sanft', 'ehrlich', 'regenbogen', 'diplom', 'geschenk', 'ehre', 'wunder', 'sonnenaufgang', 'familie', 'glücklich', 'lachen', 'paradies', 'sonne'] # 'sonne' instead of 'liebkosung'\n",
    "    unplez = ['missbrauch', 'absturz', 'schmutz', 'mord', 'krankheit', 'unfall', 'tod', 'trauer', 'gift', 'gestank', 'angriff', 'katastrophe', 'hass', 'umweltverschmutzung', 'tragödie', 'scheidung', 'gefängnis', 'armut', 'hässlich', 'krebs', 'töten', 'faul', 'erbrechen']\n",
    "\n",
    "elif LANGUAGE == \"it\":\n",
    "    \n",
    "    num_iter = 20\n",
    "    \n",
    "    # Italian\n",
    "    masc = ['confine','lido','appartamento','paio','vagone','carbone','viaggio','addome','dolore']\n",
    "    fem = ['frontiera','spiaggia','casa','coppia','carrozza','carbonella','gita','pancia','agonia']\n",
    "    man = ['uomo', 'padre', 'maschio', 'nonno', 'marito', 'zio', 'figlio','ragazzo']\n",
    "    wom = ['femmina', 'zia', 'moglie', 'donna', 'madre', 'nonna', 'ragazza','figlia']\n",
    "\n",
    "    sci = [\"astronomia\", \"matematica\", \"chimica\", \"fisica\", \"biologia\", \"geologia\", \"ingegneria\", \"statistica\", \"bioingegneria\", \"biofisica\", \"biochimica\", \"ecologia\", \"microbiologia\",\"algebra\",\"geometria\",\"telecomunicazioni\",\"computer\",\"astrofisica\"]\n",
    "    hum = ['filosofia', 'umanesimo', 'arte', 'letteratura', 'italiano', 'musica', 'storia', \"psicologia\", \"sociologia\", \"geografia\", \"antropologia\", \"teologia\", \"linguistica\", \"giornalismo\", \"archeologia\", \"danza\", \"disegno\", \"pittura\"]\n",
    "\n",
    "\n",
    "    car = ['carriera', 'azienda', 'stipendio', 'ufficio', 'esperto', 'gestione','affari', 'dirigente']\n",
    "    fam = ['matrimonio', 'nozze', 'genitori', 'parenti', 'famiglia', 'casa', 'figli', 'cugini']\n",
    "\n",
    "    boy = ['marco', 'alessandro', 'giuseppe', 'giovanni', 'roberto', 'stefano', 'francesco', 'mario', 'luigi'] \n",
    "    girl = ['anna', 'maria', 'sara', 'laura', 'giulia', 'rosa','angela', 'sofia', 'stella']\n",
    "\n",
    "    flo = ['orchidea', 'rosa', 'narciso', 'lilla', 'tulipano', 'margherita', 'giglio', 'viola', 'magnolia']\n",
    "    ins = ['pulce', 'ragno', 'cimice', 'mosca', 'tarantola', 'ape', 'scarafaggio', 'zanzara', 'calabrone']\n",
    "\n",
    "    instr = ['trombone', 'banjo', 'clarinetto', 'armonica', 'mandolino', 'tromba', 'fagotto', 'tamburo', 'arpa', 'oboe', 'tuba', 'campana', 'violino', 'clavicembalo', 'pianoforte', 'viola', 'bongo', 'flauto', 'corno', 'sassofono', 'violino']\n",
    "    wep = ['ascia', 'bastone', 'lancia', 'lancia', 'fucile', 'lancia', 'lancia', 'lancia', 'missile', 'pugnale', 'pistola', 'dinamite', 'spada', 'serbatoio', 'bomba', 'pistola', 'cannone', 'granata', 'mazza', 'fionda', 'frusta']\n",
    "\n",
    "    plez = ['libertà', 'salute', 'amore', 'pace', 'allegria', 'amico', 'cielo', 'leale', 'piacere', 'diamante', 'gentile', 'onesto', 'fortunato', 'arcobaleno', 'diploma', 'dono', 'onore', 'miracolo', 'alba', 'famiglia', 'felice', 'risate', 'paradiso']\n",
    "    unplez = ['abuso', 'crash', 'sporcizia', 'omicidio', 'malattia', 'incidente', 'morte', 'dolore', 'veleno',\n",
    "    'assalto', 'disastro', 'odio', 'inquinare', 'tragedia', 'divorzio', 'carcere', 'povertà', 'brutto', 'cancro', 'uccidere', 'marcio','vomito', 'agonia']\n",
    "\n",
    "elif LANGUAGE == \"en\":\n",
    "\n",
    "#English\n",
    "\n",
    "    man = [\"man\", \"son\", \"father\", \"boy\", \"uncle\", \"grandpa\", \"husband\", \"male\"]\n",
    "    wom = [\"mother\", \"wife\", \"aunt\", \"woman\", \"girl\", \"female\", \"grandma\", \"daughter\"]\n",
    "\n",
    "    sci = [\"astronomy\", \"math\", \"chemistry\", \"physics\", \"biology\", \"geology\", \"engineering\", \"statistics\", \"bioengineering\", \"biophysics\", \"biochemistry\", \"ecology\", \"microbiology\", \"algebra\", \"geometry\",\"telecommunications\", \"computer\", \"astrophysics\"]\n",
    "    hum = [\"history\", \"arts\", \"humanities\", \"english\", \"philosophy\", \"music\", \"literature\", \"psychology\", \"sociology\", \"geography\", \"anthropology\", \"theology\", \"linguistics\", \"journalism\",\"archaeology\",\"dancing\",\"drawing\", \"painting\"]\n",
    "\n",
    "    car = ['career', 'corporation', 'salary', 'office', 'professional', 'management', 'business', 'executive'] \n",
    "    fam = ['wedding', 'marriage', 'parents', 'relatives', 'family', 'home', 'children', 'cousins']\n",
    "    \n",
    "    boy = ['Ben', 'Paul', 'Daniel', 'John', 'Jeffrey', 'Mike','Kevin','Steve','Greg']\n",
    "    girl = ['Rebecca', 'Michelle', 'Emily', 'Julia', 'Anna','Amy','Lisa','Sarah','Kate']\n",
    "\n",
    "    flo = ['clover', 'orchid', 'rose','lilac', 'tulip', 'daisy', 'lily', 'violet', 'magnolia']\n",
    "    ins = ['ant', 'flea', 'spider','fly', 'tarantula', 'bee', 'cockroach', 'mosquito', 'hornet']\n",
    "\n",
    "    instr = ['guitar', 'lute', 'trombone', 'banjo', 'clarinet', 'harmonica', 'mandolin', 'trumpet',\n",
    "                      'bassoon', 'drum','harp','bell', 'fiddle', 'harpsichord', 'piano', 'viola', 'bongo', 'flute',\n",
    "                      'horn', 'saxophone', 'violin']\n",
    "    wep =['arrow', 'club', 'gun', 'missile', 'spear', 'axe', 'dagger', 'harpoon', 'pistol', 'sword','dynamite',\n",
    "                      'rifle','tank', 'bomb', 'firearm', 'knife', 'teargas', 'cannon', 'grenade','slingshot', 'whip']\n",
    "\n",
    "    plez  = ['freedom', 'health', 'love', 'peace', 'cheer', 'friend', 'heaven', 'loyal', 'pleasure', 'diamond',\n",
    "                     'gentle', 'honest','lucky', 'rainbow', 'diploma', 'gift', 'honor', 'miracle','family', 'happy', 'laughter',\n",
    "                     'paradise', 'vacation']\n",
    "    unplez = ['abuse','filth' , 'murder' , 'sickness' ,'death', 'grief', 'poison', 'stink', 'assault',\n",
    "                      'disaster', 'hatred','pollute', 'tragedy', 'divorce', 'jail', 'poverty', 'ugly', 'cancer', 'kill', 'rotten',\n",
    "                      'vomit', 'agony', 'prison']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the words used for tests are in the vocabulary of the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_contains_words(vocab: list, words: list):\n",
    "    vocab = set(vocab)\n",
    "    words = set([w.lower() for w in words])\n",
    "    words_len = len(words)\n",
    "    matching_words_len = len(vocab.intersection(words))\n",
    "    non_matching_words = words.difference(vocab)\n",
    "    print(f\"{matching_words_len}/{words_len} match\", end=\"\")\n",
    "    print(\"\" if len(non_matching_words) == 0 else f\", non-matching words:{non_matching_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring consistency in the length of stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 match\n",
      "***************\n",
      "9/9 match\n",
      "8/8 match\n",
      "***************\n",
      "8/8 match\n",
      "18/18 match\n",
      "***************\n",
      "18/18 match\n",
      "8/8 match\n",
      "***************\n",
      "8/8 match\n",
      "9/9 match\n",
      "***************\n",
      "9/9 match\n",
      "9/9 match\n",
      "***************\n",
      "9/9 match\n",
      "20/20 match\n",
      "***************\n",
      "16/16 match\n",
      "23/23 match\n",
      "***************\n",
      "23/23 match\n"
     ]
    }
   ],
   "source": [
    "# Check if all words are in vocab, if pairs have the same number of words\n",
    "all_word_lists = [masc, fem, man, wom, sci, hum, car, fam, boy, girl, flo, ins, instr, wep, plez, unplez]\n",
    "vocab_set = set(vocab)\n",
    "\n",
    "for i, word_list in enumerate(all_word_lists):\n",
    "    check_if_contains_words(vocab, word_list)\n",
    "    if i % 2 == 0:\n",
    "        print(\"***************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original WEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import operations\n",
    "\n",
    "def perform_weat(target1, target2, attribute1, attribute2, WEAT_gender_removed_FR_embeddings_2, my_word2id):\n",
    "    \n",
    "    myOperations = operations(10000,WEAT_gender_removed_FR_embeddings_2, my_word2id,'normal',attribute1,attribute2,target1,target2)\n",
    "    results = myOperations.perform_weat()\n",
    "    return results\n",
    "\n",
    "def cos_sim(emb1, emb2):\n",
    "    return((emb1/np.linalg.norm(emb1)).dot(emb2/np.linalg.norm(emb2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flower, insect initial WEAT results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference of means is  0.05192457067801044\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.00040373610075872257   ---  effectSize:  1.5906179166845718\n",
      "instrument,weapon initial WEAT results\n",
      "The difference of means is  0.03348056456133139\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0004649519257121648   ---  effectSize:  1.0183064925176402\n"
     ]
    }
   ],
   "source": [
    "d_gg = []\n",
    "p_gg = []\n",
    "\n",
    "d_gens = []\n",
    "p_gens = []\n",
    "\n",
    "d_genc = []\n",
    "p_genc = []\n",
    "\n",
    "test_accur = []\n",
    "train_accur = []\n",
    "gonen_same = []\n",
    "gonen_diff = []\n",
    "\n",
    "d_flo = []\n",
    "d_wep = []\n",
    "\n",
    "print(\"flower, insect initial WEAT results\")\n",
    "result = perform_weat(flo, ins, plez, unplez, my_embeddings, my_word2id)\n",
    "d_flo.append(result[1])\n",
    "print(\"instrument,weapon initial WEAT results\")\n",
    "result = perform_weat(instr, wep, plez, unplez, my_embeddings, my_word2id)\n",
    "d_wep.append(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial GG-WEAT results\n",
      "The difference of means is  0.11095569256239132\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  8.778553017951829e-05   ---  effectSize:  1.7872753522619154\n",
      "**************\n",
      "initial GenS WEAT results\n",
      "The difference of means is  -0.018170136991188142\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.9054525130985225   ---  effectSize:  -0.43557145844448714\n",
      "**************\n",
      "initial GenC WEAT results\n",
      "The difference of means is  0.06777354775083799\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.007220986127785323   ---  effectSize:  1.2283750945821663\n"
     ]
    }
   ],
   "source": [
    "print(\"initial GG-WEAT results\")\n",
    "gg_result = perform_weat(masc, fem, man, wom, my_embeddings, my_word2id)\n",
    "d_gg.append(gg_result[1])\n",
    "p_gg.append(gg_result[0])\n",
    "print(\"**************\")   \n",
    "print(\"initial GenS WEAT results\")\n",
    "gens_result = perform_weat(sci, hum, man, wom, my_embeddings, my_word2id)\n",
    "d_gens.append(gens_result[1])\n",
    "p_gens.append(gens_result[0])\n",
    "print(\"**************\")  \n",
    "print(\"initial GenC WEAT results\")\n",
    "genc_result = perform_weat(car, fam, boy, girl, my_embeddings, my_word2id)\n",
    "d_genc.append(genc_result[1])\n",
    "p_genc.append(genc_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_sim(word1, src_emb, src_id2word, word2, tgt_emb, tgt_id2word):\n",
    "    word2id_1 = {v: k for k, v in src_id2word.items()}\n",
    "    emb_1 = src_emb[word2id_1[word1]]\n",
    "    word2id_2 = {v: k for k, v in tgt_id2word.items()}\n",
    "    emb_2 = tgt_emb[word2id_2[word2]] \n",
    "    return((emb_1/np.linalg.norm(emb_1)).dot(emb_2/np.linalg.norm(emb_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading 5,000 inanimate grammatically feminine and masculine nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of total masculine nouns\n",
      "4144\n",
      "size of total feminine nouns\n",
      "3657\n",
      "size of masculine and feminine nouns (made equal)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3657"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_m_nouns = []\n",
    "expanded_f_nouns = []\n",
    "count = 0\n",
    "pth1 = \"./data/nouns/\"+LANGUAGE+\"-masc-v2.txt\"\n",
    "with open(pth1, \"r\") as f:\n",
    "    for line in f:\n",
    "        word_raw = line.strip()\n",
    "        if word_raw not in expanded_m_nouns:\n",
    "            if word_raw in my_word2id.keys():\n",
    "                expanded_m_nouns.append(word_raw)\n",
    "                \n",
    "\n",
    "print(\"size of total masculine nouns\")\n",
    "print(len(expanded_m_nouns))\n",
    "\n",
    "pth1 = \"./data/nouns/\"+LANGUAGE+\"-fem-v2.txt\"\n",
    "with open(pth1, \"r\") as f:\n",
    "    for line in f:\n",
    "        word_raw = line.strip()\n",
    "        if word_raw not in expanded_f_nouns:\n",
    "            if word_raw in my_word2id.keys():\n",
    "                expanded_f_nouns.append(word_raw)\n",
    "\n",
    "print(\"size of total feminine nouns\")\n",
    "print(len(expanded_f_nouns))\n",
    "\n",
    "#pairing them for easier processing (not semantically paired)\n",
    "grammar_pair_expanded = []\n",
    "for f,m in zip(expanded_f_nouns, expanded_m_nouns):\n",
    "    pair = [f,m]\n",
    "    grammar_pair_expanded.append(pair)\n",
    "\n",
    "print(\"size of masculine and feminine nouns (made equal)\")\n",
    "len(grammar_pair_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4144/4144 match\n",
      "3657/3657 match\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3657"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_contains_words(vocab, expanded_m_nouns)\n",
    "check_if_contains_words(vocab, expanded_f_nouns)\n",
    "len(grammar_pair_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training SVC to learn the difference between the feminine and masculine grammatical gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import normalize #machine learning algorithm library\n",
    "\n",
    "\n",
    "clf_kfold = LinearSVC()\n",
    "#selecting 3,000 feminine and 3,000 masculine nouns for learning the grammatical gender subspace\n",
    "shortened_3000 = grammar_pair_expanded[:3000]\n",
    "\n",
    "#selecting another subset for testing.\n",
    "rest = grammar_pair_expanded[3000:6000]\n",
    "X_rest = np.zeros((len(rest)*2, 300))\n",
    "\n",
    "counter = 0\n",
    "for pair in rest:\n",
    "    X_rest[counter] = my_embeddings[my_word2id[pair[0]]]\n",
    "    counter += 1\n",
    "    X_rest[counter] = my_embeddings[my_word2id[pair[1]]]\n",
    "    counter += 1\n",
    "    \n",
    "#normalizing the embeddings\n",
    "X_rest=normalize(X_rest,axis=0)\n",
    "#creating gender labels\n",
    "y_rest = np.tile([1,2],len(rest))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1314"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC can predict grammatical gender with a high accuracy. Grammatical gender direction is the coefficients of the SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial classification accuracy is 0.982\n",
      "test classification accuracy is 0.9459665144596652\n"
     ]
    }
   ],
   "source": [
    "X_3000 = np.zeros((6000, 300))\n",
    "\n",
    "counter = 0\n",
    "for pair in shortened_3000:\n",
    "    X_3000[counter] = my_embeddings[my_word2id[pair[0]]]\n",
    "    counter += 1\n",
    "    X_3000[counter] = my_embeddings[my_word2id[pair[1]]]\n",
    "    counter += 1\n",
    "    \n",
    "\n",
    "#normalizing the embeddings\n",
    "X_3000=normalize(X_3000,axis=0)\n",
    "\n",
    "#creating gender labels\n",
    "y_3000 = np.tile([1,2],3000)\n",
    "\n",
    "\n",
    "clf_3000 = LinearSVC(C = 10)\n",
    "clf_3000.fit(X_3000, y_3000)\n",
    "acc = clf_3000.score(X_3000,y_3000)\n",
    "print(\"Initial classification accuracy is\", acc)\n",
    "\n",
    "acc1 = clf_3000.score(X_rest,y_rest)\n",
    "print(\"test classification accuracy is\", acc1)\n",
    "\n",
    "test_accur.append(acc1)\n",
    "train_accur.append(acc)\n",
    "\n",
    "#selecting the decision hyperplane as the grammatical gender signal\n",
    "coef = clf_3000.coef_\n",
    "grammar_gender_direction_3000 = np.reshape(coef/np.linalg.norm(coef), (300,))\n",
    "\n",
    "gg = np.reshape(coef/np.linalg.norm(coef), (300,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projecting Out Grammatical Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "\n",
    "#function for projecting out \n",
    "def drop(u, v):\n",
    "    return u - ((v * u.dot(v)) / (v.dot(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ValNorm as valnorm\n",
    "import calcValNorm as calcValNorm\n",
    "import os\n",
    "\n",
    "def prep_input(word2id, embedding):\n",
    "\n",
    "    semanticModel = {}\n",
    "\n",
    "    for word in word2id:\n",
    "        semanticModel[word] = embedding[word2id[word]]\n",
    "\n",
    "    return semanticModel\n",
    "\n",
    "def read_vocab(file_name):\n",
    "    f = open(file_name, \"r\")\n",
    "    f.readline()\n",
    "    my_list = []\n",
    "    for line in f:\n",
    "        words = line.split(\",\")\n",
    "        my_list.append(words[1])\n",
    "\n",
    "    f.close()\n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gonen et al's Grammatical Gender Neutralization Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for reading simlex-999 noun pairs\n",
    "def read_data(file_name):\n",
    "    nouns_1 = []\n",
    "    nouns_2 = []\n",
    "    with io.open(file_name, \"r\") as f:\n",
    "        for line in f:\n",
    "            words = line[:-1].split(',')\n",
    "            nouns_1.append(words[0])\n",
    "            nouns_2.append(words[1])\n",
    "    return nouns_1, nouns_2\n",
    "\n",
    "#function for computing the avg cosine similarity among nouns with the same gender and nouns with \n",
    "#differing gender\n",
    "def avg_sim(nouns_1, nouns_2, my_embeddings):\n",
    "    avg = 0\n",
    "    count = 0\n",
    "    for i in range(len(nouns_1)):\n",
    "        w1 = nouns_1[i]\n",
    "        w2 = nouns_2[i]\n",
    "        w1_upp = w1[0].upper()+w1[1:]\n",
    "        w2_upp = w2[0].upper()+w2[1:]\n",
    "        #check if the words (in lower or uppercase) are in the embedding dictionary\n",
    "        cond1 = w1 in my_word2id or w1_upp in my_word2id \n",
    "        cond2 = w2 in my_word2id or w2_upp in my_word2id \n",
    "        \n",
    "        if cond1 and cond2:\n",
    "            count += 1\n",
    "            if w1 in my_word2id:\n",
    "                emb1 = my_embeddings[my_word2id[w1]]\n",
    "            else:\n",
    "                emb1 = my_embeddings[my_word2id[w1_upp]]\n",
    "            if w2 in my_word2id:\n",
    "                emb2 = my_embeddings[my_word2id[w2]]\n",
    "            else:\n",
    "                emb2 = my_embeddings[my_word2id[w2_upp]]\n",
    "                \n",
    "            avg += cos_sim(emb1, emb2)\n",
    "        else:\n",
    "            print(\"not found \", i)\n",
    "    print(\"number of word pairs used in gonen analysis\", count)\n",
    "    return (avg/count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading one file which contains pairs of nouns with the same gender, and another file where the pairs of nouns have differring genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "average cosine similarity among nouns with the same gender\n",
      "0.4248583767746038\n",
      "average cosine similarity among nouns with different gender\n",
      "0.40318205013433295\n"
     ]
    }
   ],
   "source": [
    "pth1 = \"data/nouns/gonen-test/\"+LANGUAGE+\"-same.txt\"\n",
    "nouns_3, nouns_4 = read_data(pth1)\n",
    "avg_it_same = avg_sim(nouns_3, nouns_4, my_embeddings)\n",
    "pth1 = \"data/nouns/gonen-test/\"+LANGUAGE+\"-diff.txt\"\n",
    "nouns_3, nouns_4 = read_data(pth1)\n",
    "avg_it_diff = avg_sim(nouns_3, nouns_4, my_embeddings)\n",
    "\n",
    "print(\"average cosine similarity among nouns with the same gender\")\n",
    "print(avg_it_same )\n",
    "gonen_same.append(avg_it_same)\n",
    "\n",
    "print(\"average cosine similarity among nouns with different gender\")\n",
    "print(avg_it_diff)\n",
    "gonen_diff.append(avg_it_diff)\n",
    "\n",
    "\n",
    "pth1 = \"data/nouns/gonen-test/\"+LANGUAGE+\"-same.txt\"\n",
    "nouns_3, nouns_4 = read_data(pth1)\n",
    "\n",
    "pth1 = \"data/nouns/gonen-test/\"+LANGUAGE+\"-diff.txt\"\n",
    "nouns_5, nouns_6 = read_data(pth1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Grammatical Gender Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing how much removing grammatical gender direction affects WEAT results and the performance of SVC in predicting grammatical gender. Removing gender direction from all 3000 inanimate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number  1 \n",
      "\n",
      "accuracy after gender removal is 0.883\n",
      "test classification accuracy is 0.5731666666666667\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.058422174551766834\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0008234253798269364   ---  effectSize:  1.4871522827890762\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.015091492634315987\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.9522298804671538   ---  effectSize:  -0.5533396052315148\n",
      "GenC WEAT results\n",
      "The difference of means is  0.0697265313077431\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0036083284680266114   ---  effectSize:  1.3502997447501066\n",
      "The difference of means is  0.05225885762283818\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0003220864033717241   ---  effectSize:  1.5916766826021274\n",
      "The difference of means is  0.03336281753241214\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0005858101988779341   ---  effectSize:  1.0069778895454848\n",
      "*********************\n",
      "iteration number  2 \n",
      "\n",
      "accuracy after gender removal is 0.7676666666666667\n",
      "test classification accuracy is 0.5546666666666666\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.05328845266982766\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.001206375342533872   ---  effectSize:  1.4130824945382563\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.014517839062612635\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.9529131052662643   ---  effectSize:  -0.563392607815419\n",
      "GenC WEAT results\n",
      "The difference of means is  0.07044842619570293\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0033292224894460043   ---  effectSize:  1.3594647880806345\n",
      "*********************\n",
      "iteration number  3 \n",
      "\n",
      "accuracy after gender removal is 0.6995\n",
      "test classification accuracy is 0.5458333333333333\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.052617293116717354\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0014735418073331674   ---  effectSize:  1.4024849223013334\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.01441430983942468\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.9528413777667337   ---  effectSize:  -0.5638321028839643\n",
      "GenC WEAT results\n",
      "The difference of means is  0.0704060241645289\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.002823582243032363   ---  effectSize:  1.3585598633567972\n",
      "*********************\n",
      "iteration number  4 \n",
      "\n",
      "accuracy after gender removal is 0.6373333333333333\n",
      "test classification accuracy is 0.5355\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.052052440544465194\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.001633150289103491   ---  effectSize:  1.3891982436771446\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.014605492150258866\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.9581614245824238   ---  effectSize:  -0.5774222114006083\n",
      "GenC WEAT results\n",
      "The difference of means is  0.07007286987467037\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0035078723969853565   ---  effectSize:  1.3527074287970635\n",
      "*********************\n",
      "iteration number  5 \n",
      "\n",
      "accuracy after gender removal is 0.6006666666666667\n",
      "test classification accuracy is 0.5301666666666667\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.052159314282268714\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0015339596496267704   ---  effectSize:  1.390011074461358\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.014626717681188398\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.9593678833421003   ---  effectSize:  -0.5771234821121572\n",
      "GenC WEAT results\n",
      "The difference of means is  0.06992091575211572\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.003565618452599173   ---  effectSize:  1.3501347215367363\n",
      "*********************\n",
      "iteration number  6 \n",
      "\n",
      "accuracy after gender removal is 0.564\n",
      "test classification accuracy is 0.5225\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.052107400758583194\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0016822915145613404   ---  effectSize:  1.385744900786839\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.014591052737710653\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.958806346114335   ---  effectSize:  -0.5765862079423137\n",
      "GenC WEAT results\n",
      "The difference of means is  0.07077112771008676\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.003229297456677882   ---  effectSize:  1.3579105983662512\n",
      "*********************\n",
      "iteration number  7 \n",
      "\n",
      "accuracy after gender removal is 0.5481666666666667\n",
      "test classification accuracy is 0.5183333333333333\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.05209575177915951\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0017911510753898519   ---  effectSize:  1.3819774649655783\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.0147344133413988\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.9582271313455836   ---  effectSize:  -0.5806299932892416\n",
      "GenC WEAT results\n",
      "The difference of means is  0.07076536011443785\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0033465465362699787   ---  effectSize:  1.356335509782755\n",
      "*********************\n",
      "iteration number  8 \n",
      "\n",
      "accuracy after gender removal is 0.5465\n",
      "test classification accuracy is 0.5178333333333334\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.05226500925596437\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0017946308682179613   ---  effectSize:  1.3816702473351428\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.014815485076582929\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.9594293786167972   ---  effectSize:  -0.5818883810047958\n",
      "GenC WEAT results\n",
      "The difference of means is  0.07109157424194512\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0034315165805839376   ---  effectSize:  1.3589148381213303\n",
      "*********************\n",
      "iteration number  9 \n",
      "\n",
      "accuracy after gender removal is 0.548\n",
      "test classification accuracy is 0.517\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.05224325549097007\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0019229571829488812   ---  effectSize:  1.3820261501801383\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.014772894983533699\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.9606239031389019   ---  effectSize:  -0.580465976465034\n",
      "GenC WEAT results\n",
      "The difference of means is  0.07107485910690295\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.003474249683781072   ---  effectSize:  1.3588800760770774\n",
      "*********************\n",
      "iteration number  10 \n",
      "\n",
      "accuracy after gender removal is 0.5416666666666666\n",
      "test classification accuracy is 0.5141666666666667\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.05148416497632739\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0015955502992920367   ---  effectSize:  1.386933901307014\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.013709613988179215\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.9510413295595481   ---  effectSize:  -0.544519009224641\n",
      "GenC WEAT results\n",
      "The difference of means is  0.07074318461520167\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0034089566867936405   ---  effectSize:  1.3605998952368727\n",
      "*********************\n",
      "iteration number  11 \n",
      "\n",
      "accuracy after gender removal is 0.5368333333333334\n",
      "test classification accuracy is 0.5131666666666667\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.05075077559109504\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0015577918390777823   ---  effectSize:  1.4151515818657048\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.012232912625366027\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.9340069849687658   ---  effectSize:  -0.49692708697965\n",
      "GenC WEAT results\n",
      "The difference of means is  0.06835897309123326\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.00357932459692778   ---  effectSize:  1.3487826066412851\n",
      "*********************\n",
      "iteration number  12 \n",
      "\n",
      "accuracy after gender removal is 0.5288333333333334\n",
      "test classification accuracy is 0.5098333333333334\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.04969665952806817\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0012587121243318178   ---  effectSize:  1.4256484878215725\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.010979906340694618\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.9111579622393172   ---  effectSize:  -0.4457432330538316\n",
      "GenC WEAT results\n",
      "The difference of means is  0.06506205810335443\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.004325962570807973   ---  effectSize:  1.316459670632825\n",
      "*********************\n",
      "iteration number  13 \n",
      "\n",
      "accuracy after gender removal is 0.526\n",
      "test classification accuracy is 0.5106666666666667\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.04685977478623049\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0016457103893384728   ---  effectSize:  1.3988497125517454\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.008713570039552235\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.8551696718289201   ---  effectSize:  -0.352577195029591\n",
      "GenC WEAT results\n",
      "The difference of means is  0.06452560571664182\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.004425263411729219   ---  effectSize:  1.307176454469571\n",
      "*********************\n",
      "iteration number  14 \n",
      "\n",
      "accuracy after gender removal is 0.5283333333333333\n",
      "test classification accuracy is 0.5085\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.04459075243732465\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0016303887211872592   ---  effectSize:  1.3695894768013108\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.00783354273828416\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.8250771511479504   ---  effectSize:  -0.31361623877648437\n",
      "GenC WEAT results\n",
      "The difference of means is  0.06398479162833651\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.004890418678663466   ---  effectSize:  1.297286929518571\n",
      "*********************\n",
      "iteration number  15 \n",
      "\n",
      "accuracy after gender removal is 0.5191666666666667\n",
      "test classification accuracy is 0.508\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.043948751180328705\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0019455114479338675   ---  effectSize:  1.3652490586793542\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.008110510884548904\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.8273567302260583   ---  effectSize:  -0.3216534612681211\n",
      "GenC WEAT results\n",
      "The difference of means is  0.0635873409602381\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.004813194771924323   ---  effectSize:  1.2864651646273857\n",
      "*********************\n",
      "iteration number  16 \n",
      "\n",
      "accuracy after gender removal is 0.5198333333333334\n",
      "test classification accuracy is 0.5081666666666667\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.04391428248936391\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0019131695656225967   ---  effectSize:  1.3684269670717404\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.00843330652531807\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.8385072039068283   ---  effectSize:  -0.33471541782643094\n",
      "GenC WEAT results\n",
      "The difference of means is  0.06362175536192952\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.005308333642046192   ---  effectSize:  1.2859796628889215\n",
      "*********************\n",
      "iteration number  17 \n",
      "\n",
      "accuracy after gender removal is 0.5138333333333334\n",
      "test classification accuracy is 0.5108333333333334\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.043888769085162864\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.001964130921747742   ---  effectSize:  1.3707295294071518\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.008714943746055116\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.8481431664721779   ---  effectSize:  -0.34604606981494296\n",
      "GenC WEAT results\n",
      "The difference of means is  0.06364976282228015\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.005207110586007202   ---  effectSize:  1.2854521155473089\n",
      "*********************\n",
      "iteration number  18 \n",
      "\n",
      "accuracy after gender removal is 0.5063333333333333\n",
      "test classification accuracy is 0.511\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.044001222813448725\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.001808230672939004   ---  effectSize:  1.3675159282557985\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.008137465119594199\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.8267915663267884   ---  effectSize:  -0.32299248393851165\n",
      "GenC WEAT results\n",
      "The difference of means is  0.06314879922654307\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.005493147129709497   ---  effectSize:  1.277719224330442\n",
      "*********************\n",
      "iteration number  19 \n",
      "\n",
      "accuracy after gender removal is 0.515\n",
      "test classification accuracy is 0.5078333333333334\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.04410574059648982\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.001976268986705554   ---  effectSize:  1.3628208988943482\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.007120271612955445\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.8074566747562779   ---  effectSize:  -0.28760216670359384\n",
      "GenC WEAT results\n",
      "The difference of means is  0.06277528152404345\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.004832629515144071   ---  effectSize:  1.285764065205969\n",
      "*********************\n",
      "iteration number  20 \n",
      "\n",
      "accuracy after gender removal is 0.5051666666666667\n",
      "test classification accuracy is 0.5133333333333333\n",
      "number of word pairs used in gonen analysis 203\n",
      "not found  12\n",
      "not found  83\n",
      "not found  90\n",
      "not found  102\n",
      "not found  118\n",
      "not found  183\n",
      "not found  210\n",
      "not found  222\n",
      "number of word pairs used in gonen analysis 318\n",
      "GG-WEAT results\n",
      "The difference of means is  0.044411944642849435\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0018765757120660087   ---  effectSize:  1.3613058594103622\n",
      "GenS WEAT results\n",
      "The difference of means is  -0.0067717764367734605\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.790399183262282   ---  effectSize:  -0.27445832664828573\n",
      "GenC WEAT results\n",
      "The difference of means is  0.0636974416740619\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0051683382260993005   ---  effectSize:  1.2886436977842826\n",
      "*********************\n"
     ]
    }
   ],
   "source": [
    "WEAT_gender_removed_embeddings_2 = np.zeros((len(my_word2id),300))\n",
    " \n",
    "    \n",
    "for i in range(len(my_word2id)):\n",
    "    WEAT_gender_removed_embeddings_2[i] = my_embeddings[i]\n",
    "\n",
    "for j in range(num_iter):\n",
    "    print(\"iteration number \", j+1, \"\\n\")\n",
    "\n",
    "    #projecting out grammatical gender for all of words\n",
    "    for word in my_word2id:\n",
    "        word_emb = WEAT_gender_removed_embeddings_2[my_word2id[word]]\n",
    "        WEAT_gender_removed_embeddings_2[my_word2id[word]] = drop(u=word_emb, v=grammar_gender_direction_3000)        \n",
    "\n",
    "\n",
    "    X_3000_after = np.zeros((6000, 300))\n",
    "    y_3000_after = np.tile([1,2],3000)\n",
    "    counter = 0\n",
    "    #obtaining new embeddings for inanimate nouns\n",
    "    for pair in shortened_3000:\n",
    "        X_3000_after[counter] = WEAT_gender_removed_embeddings_2[my_word2id[pair[0]]]\n",
    "        counter += 1\n",
    "        X_3000_after[counter] = WEAT_gender_removed_embeddings_2[my_word2id[pair[1]]]\n",
    "        counter += 1\n",
    "    \n",
    "    X_rest_after = np.zeros((6000, 300))\n",
    "    y_rest_after = np.tile([1,2],3000)\n",
    "    counter = 0\n",
    "    #obtaining new embeddings for test nouns\n",
    "    for pair in rest:\n",
    "        X_rest_after[counter] = WEAT_gender_removed_embeddings_2[my_word2id[pair[0]]]\n",
    "        counter += 1\n",
    "        X_rest_after[counter] = WEAT_gender_removed_embeddings_2[my_word2id[pair[1]]]\n",
    "        counter += 1\n",
    "    \n",
    "    #training SVC to learn grammatical gender hyperplane\n",
    "    clf_3000_after = LinearSVC(C = 10)\n",
    "    clf_3000_after.fit(X_3000_after, y_3000_after)\n",
    "    \n",
    "    accuracy = clf_3000_after.score(X_3000_after,y_3000_after)\n",
    "    print(\"accuracy after gender removal is\", accuracy)\n",
    "    train_accur.append(accuracy)\n",
    "    \n",
    "    acc1 =  clf_3000_after.score(X_rest_after,y_rest_after)\n",
    "    print(\"test classification accuracy is\", acc1)\n",
    "    test_accur.append(acc1)\n",
    "    \n",
    "    #obtaining the new hyperplane\n",
    "    coef_after = clf_3000_after.coef_\n",
    "    grammar_gender_direction_3000= np.reshape(coef_after/np.linalg.norm(coef_after), (300,))\n",
    "    \n",
    "    #gonen et al. computations\n",
    "    avg_same = avg_sim(nouns_3, nouns_4, WEAT_gender_removed_embeddings_2)\n",
    "    avg_diff = avg_sim(nouns_5, nouns_6, WEAT_gender_removed_embeddings_2)\n",
    "    gonen_same.append(avg_same)\n",
    "    gonen_diff.append(avg_diff)\n",
    "\n",
    "#     #gg-weat computation\n",
    "    print(\"GG-WEAT results\")\n",
    "    gg_result = perform_weat(masc, fem, man, wom, WEAT_gender_removed_embeddings_2, my_word2id)\n",
    "    d_gg.append(gg_result[1])\n",
    "    p_gg.append(gg_result[0])\n",
    "    \n",
    "#     #genS computation\n",
    "    print(\"GenS WEAT results\")\n",
    "    gens_result = perform_weat(sci, hum, man, wom, WEAT_gender_removed_embeddings_2, my_word2id)\n",
    "    d_gens.append(gens_result[1])\n",
    "    p_gens.append(gens_result[0])\n",
    "    \n",
    "#     #genC computation\n",
    "    print(\"GenC WEAT results\")\n",
    "    genc_result = perform_weat(car, fam, boy, girl, WEAT_gender_removed_embeddings_2, my_word2id)\n",
    "    d_genc.append(genc_result[1])\n",
    "    p_genc.append(genc_result[0])\n",
    "    \n",
    "#     #Baseline WEAT\n",
    "    #perform baseline weat only after the first iteration of GG removal\n",
    "    if j == 0:\n",
    "        result = perform_weat(flo, ins, plez, unplez, WEAT_gender_removed_embeddings_2, my_word2id)\n",
    "        d_flo.append(result[1])\n",
    "        result = perform_weat(instr, wep, plez, unplez, WEAT_gender_removed_embeddings_2, my_word2id)\n",
    "        d_wep.append(result[1])\n",
    "    \n",
    "    print(\"*********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(f\"{BASE_DIR}/results/train/{LANGUAGE}-acc.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"iteration\", \"test_accuracy\"])\n",
    "    for i, acc in enumerate(test_accur):\n",
    "        writer.writerow([i, acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the training statistics and save them to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'iter':list(range(0,num_iter+1)),\n",
    "        'train_acc':train_accur,\n",
    "        'test_acc':test_accur,\n",
    "        'GenS':d_gens,\n",
    "        'P_GenS':p_gens,\n",
    "        'GenC':d_genc,\n",
    "       'P_GenC':p_genc,\n",
    "       'GG':d_gg,\n",
    "       'P_GG':p_gg,\n",
    "       'gonen_same':gonen_same,\n",
    "       'gonen_diff': gonen_diff}\n",
    "  \n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = {\n",
    "    'data': [masc] + [fem] + [man] + [wom] + [boy] + [girl] + [sci] + [hum] + [car] + [fam] + [plez] + [unplez] + [flo] + [ins] + [instr] +[wep]\n",
    "}\n",
    "config_df = pd.DataFrame(config_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference of means is  0.049774652874629705\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  0.0006526150975197931   ---  effectSize:  1.51710332771042\n",
      "The difference of means is  0.04445868046451618\n",
      "Generating null distribution...\n",
      "Number of permutations  10000\n",
      "Getting the entire distribution\n",
      "p-value:  2.4676360742548198e-05   ---  effectSize:  1.244971218527566\n"
     ]
    }
   ],
   "source": [
    "result = perform_weat(flo, ins, plez, unplez, WEAT_gender_removed_embeddings_2, my_word2id)\n",
    "d_flo.append(result[1])\n",
    "result = perform_weat(instr, wep, plez, unplez, WEAT_gender_removed_embeddings_2, my_word2id)\n",
    "d_wep.append(result[1])\n",
    "\n",
    "\n",
    "baseline_data = {\n",
    "    'flow_ins': d_flo,\n",
    "    'instr_wep':d_wep\n",
    "}\n",
    "baseline_df = pd.DataFrame(baseline_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_excel_filename = f\"{BASE_DIR}/results/train/{LANGUAGE}.xlsx\"\n",
    "with pd.ExcelWriter(output_excel_filename) as writer:  \n",
    "\n",
    "    df.to_excel(writer, sheet_name='results')\n",
    "    config_df.to_excel(writer, sheet_name='stimuli')\n",
    "    baseline_df.to_excel(writer, sheet_name='baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(DEBIASED_PATH, vocab, WEAT_gender_removed_embeddings_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
